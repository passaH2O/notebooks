{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GeoFlood_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/passaH2O/notebooks/blob/main/GeoFlood_demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GeoFlood demo Jupyter notebook**\n",
        "\n",
        "**Save a copy of this notebook on Drive**\n",
        "\n",
        " - Before you do anything, go to File > Save copy in Drive so you can keep and work on your own copy.*\n",
        "\n",
        "**Purpose of this notebook**\n",
        " - Perform all operations in the GeoFlood workflow to obtain flood inundation extent and depth based on a hydrological forecast of discharge.\n",
        "\n",
        "<sub>*This notebook is intended to be run on Google Colab, but can be run locally with some minor modifications."
      ],
      "metadata": {
        "id": "1LXi4paalFZa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS1ubCZ_fuXt"
      },
      "source": [
        "---\n",
        "# **(1) Prepare the GeoFlood software environment in Google Colab**\n",
        " - Install GRASS GIS and GDAL\n",
        " - Clone GeoFlood and TauDEM GitHub repositories\n",
        " - Download necessary python libraries\n",
        " - Allow ~10 minutes to for this cell to run"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%%bash\n",
        "\n",
        "# add ppas for latest package versions\n",
        "sudo add-apt-repository --yes ppa:ubuntugis/ppa \n",
        "sudo add-apt-repository --yes ppa:ubuntu-toolchain-r/test\n",
        "sudo apt update && sudo apt-get upgrade --yes\n",
        "\n",
        "# download and install GRASS GIS and GDAL\n",
        "sudo apt install --yes --fix-missing libproj-dev proj-data proj-bin \\\n",
        "                 libgeos-dev unzip subversion grass grass-dev \\\n",
        "                 libcanberra-gtk-module python3-distutils \\\n",
        "                 libhdf5-serial-dev netcdf-bin libnetcdf-dev libnetcdff-dev\n",
        "\n",
        "# download and install geoflood\n",
        "git clone https://github.com/passaH2O/GeoFlood.git\n",
        "\n",
        "# install GDAL separately because it needs special arguments to setup correctly\n",
        "pip install \\\n",
        "  --global-option=build_ext \\\n",
        "  --global-option=\"-I/usr/include/gdal\" \\\n",
        "  GDAL==`gdal-config --version`\n",
        "\n",
        "# write textfile with all the python libraries we will install\n",
        "cat <<EOT >> geoflood_requirements.txt\n",
        "numpy\n",
        "scipy\n",
        "pandas\n",
        "matplotlib\n",
        "ipykernel\n",
        "rasterio==1.1.3\n",
        "numba\n",
        "netCDF4\n",
        "statsmodels\n",
        "proj\n",
        "six\n",
        "geopandas\n",
        "EOT\n",
        "\n",
        "# install GeoFlood python dependencies\n",
        "pip install -r geoflood_requirements.txt\n",
        "\n",
        "# download and install taudem\n",
        "# NOTE: in this notebook, we use an experimental fork of TauDEM\n",
        "git clone https://github.com/amoodie/TauDEM.git\n",
        "cd TauDEM\n",
        "git checkout amoodie-inunmap\n",
        "\n",
        "# get colab's number of cores (probably 2)\n",
        "export nproc=$(nproc)\n",
        "\n",
        "# compile and build TauDEM\n",
        "cd src\n",
        "mkdir build\n",
        "cd build\n",
        "cmake ..\n",
        "make -j $nproc && make install -j $nproc"
      ],
      "metadata": {
        "id": "chk7Ufwqm2-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " - add TauDEM executables to PATH environment variable"
      ],
      "metadata": {
        "id": "A-XpMMLEoITS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PATH'] = '/usr/local/taudem:' + os.environ['PATH']"
      ],
      "metadata": {
        "id": "-SRFXmptSnQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E74Dff5h6001"
      },
      "source": [
        "---\n",
        "# **(2) Configuration of folders for storing and accessing inputs and outputs**\n",
        "First run the cell below and follow the prompts to mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LtLzjlH2VBD"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Make a folder in Google Drive called `geoflood_demo` to store input and output data for this demo notebook\n",
        "2. Download example input data to `geoflood_demo`"
      ],
      "metadata": {
        "id": "pEYMHNYga4Xl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "!wget https://github.com/passaH2O/notebooks/raw/main/INPUT.tar.gz\n",
        "!tar xzf INPUT.tar.gz INPUT 2> /dev/null\n",
        "!find INPUT -name '.*' -type f -delete\n",
        "%mkdir /content/drive/MyDrive/geoflood_demo\n",
        "%mv INPUT /content/drive/MyDrive/geoflood_demo"
      ],
      "metadata": {
        "id": "LC5El6lRHWZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This configuration script sets up the GeoFlood file structure in our `geoflood_demo` folder"
      ],
      "metadata": {
        "id": "uosJ0fLFHY9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/GeoFlood/GeoNet/pygeonet_configure.py \\\n",
        "  -dir /content/drive/MyDrive/geoflood_demo \\\n",
        "  -p OnionCreek -n OC1mTest --no_chunk \\\n",
        "  --input_dir INPUT --output_dir OUTPUT\n"
      ],
      "metadata": {
        "id": "dqxHjnrp7t9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVTEgn8Q_Sgg"
      },
      "source": [
        "---\n",
        "# **(3) Run nonlinear filtering on a portion of Onion Creek**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkN9HqME_Y56"
      },
      "source": [
        "!python ./GeoFlood/GeoNet/pygeonet_nonlinear_filter.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XK3A_UYzCh7"
      },
      "source": [
        "---\n",
        "# **(4) Compute flow directions and flow accumulation**\n",
        "We are now going to use GRASS GIS functions to compute flow directions, flow accumulation, and identify subbasins within our domain based on this information. All these operations are contained within the script `pygeonet_grass_py3.py`. As usual, you can open it from your cloned repo on the left to have a look. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sesc_YK3zKIv"
      },
      "source": [
        "!python ./GeoFlood/GeoNet/pygeonet_grass_py3.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IgMWcYh7TmsN"
      },
      "source": [
        "---\n",
        "# **(5) Compute slope and geometric curvature on the filtered terrain**\n",
        "\n",
        "We are now going to call as input our filtered landscape and we will compute the slope and geometric curvature, which is the default definition of curvature used in GeoNet and GeoFlood. All the operations are contained within `pygeonet_slope_curvature.py`. You can open the actual script by double clicking on it from the repository on the left."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJ14Z9mqUH4m"
      },
      "source": [
        "!python ./GeoFlood/GeoNet/pygeonet_slope_curvature.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA3pxUj6UM-b"
      },
      "source": [
        "---\n",
        "# **(6) Compute skeletons based on curvature, flow accumulation, and curvature and flow accumulation combined**\n",
        "\n",
        "We will now call a new operation in this notebook, which allows us to use curvature only, flow accumulation only, and curvature and flow accumulation combined to identify which parts of the landscape are likely channelized. All these operations are contained within `pygeonet_skeleton_definition.py`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rp1uML3NU6BO"
      },
      "source": [
        "!python ./GeoFlood/GeoNet/pygeonet_skeleton_definition.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZpmK1TITBHX"
      },
      "source": [
        "---\n",
        "# **(7) Identify end points of NHD MR channels**\n",
        "\n",
        "We are going to now use `Network_Node_reading.py` to automatically identify the end points of the input flowline - the NHD MR information for this watershed. This operation is contained within the GeoFlood portion of the code and will generate a new output in the Output folder called `OC1mTest_endPoints`. This file contains the coordinates (start and end point) of the end points of each flowline. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OEA0gD5Tb5B"
      },
      "source": [
        "!python ./GeoFlood/GeoFlood/Network_Node_Reading.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkptfiyJlBbW"
      },
      "source": [
        "---\n",
        "# **(8) Network extraction with cost function including negative height**\n",
        "\n",
        "We are now going to extract the geodesic curve between end points using the default cost function that includes negative height.  \n",
        "\n",
        "**Compute negative height information**\n",
        "\n",
        "We are going to use `Relative_Height_Estimation.py`, which will return a binary raster/array with values of 1 given to pixels at a lower elevation than the elevation associated with NHD MR Flowline pixels and a value of zero to all other pixels, i.e. pixels at a higher elevation than the NHD MR Flowlines. \n",
        "\n",
        "This operation will produce three more outputs; the `_NegaHand` is the relative height we are looking for to be added to the cost function, while the other two (`_nhdflowline` and `_Allocation`) are the rasterized NHD flowline used for calculation and another term used for identifying the corridor of values below the elevation the NHD.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96m3Nu1_m_NT"
      },
      "source": [
        "!python ./GeoFlood/GeoFlood/Relative_Height_Estimation.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8yVVs0XnliP"
      },
      "source": [
        "---\n",
        "# **Network extraction with negative height**\n",
        "\n",
        "We can now run our `Network_Extraction.py` to obtain the geodesic curve between the end points. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84mvlMtFn1yn"
      },
      "source": [
        "!python ./GeoFlood/GeoFlood/Network_Extraction.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# **(9) TauDEM preparation**\n",
        "Let's assign our working directory to a variable to shorten the file paths in our TauDEM commands"
      ],
      "metadata": {
        "id": "fSG90g96wgS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%env demo=/content/drive/MyDrive/geoflood_demo"
      ],
      "metadata": {
        "id": "DdyGFX9vw-vX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TauDEM can leverage a computer's resources by running parallel processes.\n",
        "We see from the command below that Google Colab provides 2 processors, so we will run TauDEM operations with 2 processors\n",
        "\n",
        "(pass an argument of 2 with the `-n` flag)"
      ],
      "metadata": {
        "id": "ttGOROEDuV5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nproc"
      ],
      "metadata": {
        "id": "OzTAHcEuw1Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_Dxsnv7oJ64"
      },
      "source": [
        "---\n",
        "# **(10) TauDEM operations to prepare for the HAND raster calculation**\n",
        "\n",
        "We will now compute the HAND raster. As it is based on Dinf flow directions, we will need to perform pit filling on the elevation data and compute flow directions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3o5hkvo6vFA"
      },
      "source": [
        "---\n",
        " - **Pit filling**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdY4OlNe6Ryv"
      },
      "source": [
        "!mpiexec --allow-run-as-root -n 2 pitremove \\\n",
        "    -z $demo/INPUT/GIS/OnionCreek/OC1mTest.tif \\\n",
        "    -fel $demo/OUTPUT/GIS/OnionCreek/OC1mTest_fel.tif"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiVKbCId61Wj"
      },
      "source": [
        "---\n",
        " - **Dinf flow directions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zijBtlKr6WK4"
      },
      "source": [
        "!mpiexec --allow-run-as-root -n 2 dinfflowdir \\\n",
        "    -fel $demo/OUTPUT/GIS/OnionCreek/OC1mTest_fel.tif \\\n",
        "    -ang $demo/OUTPUT/GIS/OnionCreek/OC1mTest_ang.tif \\\n",
        "    -slp $demo/OUTPUT/GIS/OnionCreek/OC1mTest_slp.tif"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgMiRiz_67LH"
      },
      "source": [
        "---\n",
        "# **(11) HAND calculation with GeoFlood flowline**\n",
        "\n",
        "We will now use the previous outputs from TauDEM to compute the HAND raster based on the GeoFlood flowline. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uIJe3n26-EF"
      },
      "source": [
        "!mpiexec --allow-run-as-root -n 2 dinfdistdown \\\n",
        "    -ang $demo/OUTPUT/GIS/OnionCreek/OC1mTest_ang.tif \\\n",
        "    -fel $demo/OUTPUT/GIS/OnionCreek/OC1mTest_fel.tif \\\n",
        "    -slp $demo/OUTPUT/GIS/OnionCreek/OC1mTest_slp.tif \\\n",
        "    -src $demo/OUTPUT/GIS/OnionCreek/OC1mTest_path.tif \\\n",
        "    -dd $demo/OUTPUT/GIS/OnionCreek/OC1mTest_hand_GeoFlood.tif \\\n",
        "    -m ave v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xsU-Y0H4AV7"
      },
      "source": [
        "---\n",
        "# **(12) Calculation of hydraulic geometry for the stream reach**\n",
        "\n",
        "Now that we have the HAND raster, we will go back to GeoFlood functions to compute the hydraulic geometry properties for this river reach, which will then be used to translate the discharge forecast into a depth prediction for inundation mapping (in the next lab).\n",
        "\n",
        "The first operation we are going to perform is the segmentation of the flowline which is done within `Streamline_Segmentation.py`. This operation checks the length of the flowline; if the length is longer than a predefined maximum segment length, the script breaks the original flowline into equal-length segments with a length shorter than the predefined threshold. The input of this script is the extracted river network shapefile (`OC1mTest_channelNetwork.shp`), and the output is a river segment shapefile (`OC1mTest_channelSegment.shp`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eTj6OX6j4Vde"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **Channel segmentation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj1rKSF44a2R"
      },
      "source": [
        "!python ./GeoFlood/GeoFlood/Streamline_Segmentation.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPwYMy1g4gOE"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "# **Catchment delineation**\n",
        "\n",
        "Now we will delineate the catchment for the segmented flowline. Similarly to what we have done in the past, the script uses a GRASS function called from `Segment_Catchment_Delineation.py`.\n",
        "\n",
        "The river segment shapefile (`OC1mTest_channelSegment.shp`) is one of the script inputs. Since this script relies on the GRASS GIS catchment delineation routine, the flow direction raster (`OC1mTest_fdr.tif`) we derived with GRASS is another input. The output of this script is a segment catchment geotiff file (`OC1mTest_segmentCatchment.tif`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcB80-F14lUM"
      },
      "source": [
        "!python ./GeoFlood/GeoFlood/Grass_Delineation_py3.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbNsIhvb4128"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# **Calculation of hydraulic geometry properties**\n",
        "\n",
        "We can now use the segments, the delineated catchments, and the HAND raster to obtain the hydraulic geometry properties of each of these segments. We will start by running the GeoFlood script `River_Attribute_Estimation.py`, which is used to compute bed slope, river length, and catchment area. The inputs to this script are the DEM in the input folder, the river segments we have previously identified, and the segment catchment raster we just got from GRASS in our `OUTPUT` folder. This script will output a segment catchment polygon shapefile and a river attribute text file, which will be placed in the Hydraulics folder in your `OUTPUT/Hydraulics` folder.\n",
        "\n",
        "You will find new outputs in your Outputs folder. In particular, in the GIS folder you will find the _segmentCatchment.shp and in the Hydraulics folder you will find the river attribute text file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxvCiB-d46rh"
      },
      "source": [
        "!python ./GeoFlood/GeoFlood/River_Attribute_Estimation.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cw9oEk2498T"
      },
      "source": [
        "---\n",
        "# **Associate channel roughness to segments**\n",
        "We will now run the script `Network_Mapping.py` which takes the channel roughness information from the original river network and saves it in the segmented river network. The script takes as input the original catchment shapefile, which is the new input we just added as part of this lab (`Catchment.shp`) and the river segment shapefile we have created (`OC1mTest_channelSegment.shp`).\n",
        "\n",
        "This script will create a new output in your `OUTPUT` GIS folder which has csv format and contains the COMID of each of the segments in our flowline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKcAnChz5Cr1"
      },
      "source": [
        "!python ./GeoFlood/GeoFlood/Network_Mapping.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wqNVZg_5Kb8"
      },
      "source": [
        "---\n",
        "# **Derivation of channel hydraulic properties**\n",
        "The next script is part of TauDEM and is called CatchHydroGeo. This script is used to derive the channel hydraulic properties using the terrain analysis products. The inputs are the HAND raster we computed (`OC1mTest_HAND.tif`), the segment catchment raster (`OC1mTest_segmentCatchment.tif`), the Dinf slope raster (`OC1mTest_slp.tif`), the river attribute text file (`OC1mTest_River_Attribute.txt`) and the stage range text file we input.\n",
        "\n",
        "Once the execution is finished, the resulting channel characteristic table with hydraulic properties directly derived from the HAND raster (`hydroprop-basetable.csv`) will be found in the `OUTPUT/Hydraulics` folder and contains the hydraulic geometry properties for each stage value we input."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13O0Fqx25PhW"
      },
      "source": [
        "!mpiexec --allow-run-as-root -n 2 catchhydrogeo \\\n",
        "    -hand $demo/OUTPUT/GIS/OnionCreek/OC1mTest_hand_GeoFlood.tif \\\n",
        "    -catch $demo/OUTPUT/GIS/OnionCreek/OC1mTest_segmentCatchment.tif \\\n",
        "    -catchlist $demo/OUTPUT/Hydraulics/OnionCreek/OC1mTest_River_Attribute.txt \\\n",
        "    -slp $demo/OUTPUT/GIS/OnionCreek/OC1mTest_slp.tif \\\n",
        "    -h $demo/INPUT/Hydraulics/OnionCreek/stage.txt \\\n",
        "    -table $demo/OUTPUT/Hydraulics/OnionCreek/hydroprop-basetable.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHbNVAg85TSy"
      },
      "source": [
        "---\n",
        "# **Derivation of synthetic rating curves**\n",
        "We have a final GeoFlood script to run, called `Hydraulic_Property_Postprocess.py`, which combines the channel roughness coefficient and the properties that we have obtained in the above steps to derive reach average geometric properties for the reach and synthetic rating curves. The inputs are the COMID-based Manningâ€™s values (`COMID_Roughness.csv`), the network ID mapping csv table (`OC1mTest_networkMapping.csv`), and the base geometric properties (`hydroprop-basetable.csv`).\n",
        "\n",
        "You will find as output the full channel properties table with synthetic rating curves in the `OUTPUT` Hydraulics folder, called `hydroprop-fulltable.csv`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mvUVk2b5YfA"
      },
      "source": [
        "!python ./GeoFlood/GeoFlood/Hydraulic_Property_Postprocess.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pipY123QHXxI"
      },
      "source": [
        "# **(13) Estimation of flood inundation extent and depth**\n",
        "\n",
        "And finally, what we were all waiting for! We will integrate all the results above to estimate flood inundation extent and depth based on the hydrological prediction provided as input which comes from the National Water Model. \n",
        "\n",
        "https://water.noaa.gov/about/nwm\n",
        "\n",
        "---\n",
        "# **Assign NWM input forecasted discharge to channel segments**\n",
        "\n",
        "This first operation takes the NWM forecast that we provided in input and will assign it to each channel segment based on COMID. The script performs the discharge-stage conversion based on the NWM netCDF output (general format: `nwm.txxz.product_type.channel_rt.fxxx.conus.nc`) from the NWM input folder. The script also uses the network id mapping table, which records how the COMIDs are linked to the ids of the extracted river segments. The script also uses the full channel property table, which contains stage-discharge data pairs for each segment used in stage height interpolation. \n",
        "\n",
        "The script produces the depth corresponding to the discharge for each segment in both netCDF and CSV formats. The depths files have the same file name as the NWM inputs and they will be stored in the corresponding OUPUT NWM folder. \n",
        "\n",
        "Since our stream does not flood very often we are going to overwrite the discharge forecast so that we can make this system flood. \n",
        "\n",
        "<mark>**Modify forecasted discharge in Foreast_Table.py**</mark>\n",
        "\n",
        "Open `Forecast_Table.py` from the left menu - the script is stored within the GeoFlood folder. Double click on the name of the script and uncomment line 63 by removing the symbol `#`. As you can see we are imposing a discharge of 500 to obtain a flood inundation map (do not worry about any of the symbols that are written on that same line after 500). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuB9GkGDJA9f"
      },
      "source": [
        "!python ./GeoFlood/GeoFlood/Forecast_Table.py \\\n",
        "    $demo/INPUT/NWM/OnionCreek/nwm.t00z.analysis_assim.channel_rt.tm01.conus.nc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmkxJ5C_dhdg"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "# **Compute flood inundation map corresponding to the forecasted discharge**\n",
        "\n",
        "We will now quantify the flood extent and depth by using TauDEM's InunMap. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GJFXVo-f-bF"
      },
      "source": [
        "!mpiexec --allow-run-as-root -n 2 inunmap \\\n",
        "         -hand $demo/OUTPUT/GIS/OnionCreek/OC1mTest_hand_GeoFlood.tif \\\n",
        "         -catch $demo/OUTPUT/GIS/OnionCreek/OC1mTest_segmentCatchment.tif \\\n",
        "         -forecast $demo/OUTPUT/NWM/OnionCreek/nwm.t00z.analysis_assim.channel_rt.tm01.conus.nc \\\n",
        "         -mapfile $demo/OUTPUT/Inundation/OnionCreek/OC1mTest_NWM_inunmap.tif"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ki4fp2gTP4G"
      },
      "source": [
        "# **(14) Plot the inundation map!**\n",
        "\n",
        "We can now have a look at the results here and check our inundation map. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PiwJSlgT3Y9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import rasterio\n",
        "import rasterio.plot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0O0YOmpT7eJ"
      },
      "source": [
        "# open the HAND raster\n",
        "demo_dir = '/content/drive/MyDrive/geoflood_demo'\n",
        "with rasterio.open(f'{demo_dir}/OUTPUT/GIS/OnionCreek/OC1mTest_hand_GeoFlood.tif') as src:\n",
        "    oc_hand_image = src.read(1) # read band 1 into a numpy array\n",
        "\n",
        "# open the inundation raster\n",
        "with rasterio.open(f'{demo_dir}/OUTPUT/Inundation/OnionCreek/OC1mTest_NWM_inunmap.tif') as src:\n",
        "    oc_inun_image = src.read(1) # read band 1 into a numpy array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBOQGK1QVW5u"
      },
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(16,5))\n",
        "\n",
        "hand_im = ax[0].imshow(oc_hand_image, cmap='Blues_r', vmin=0, vmax=80)\n",
        "fig.colorbar(hand_im, ax=ax[0], shrink=0.4)\n",
        "ax[0].set_title(\"GeoFlood HAND\")\n",
        "ax[0].set_xlabel('x (m)')\n",
        "ax[0].set_ylabel('y (m)')\n",
        "\n",
        "inun_im = ax[1].imshow(oc_inun_image, cmap='Blues_r', vmin=0, vmax=8)\n",
        "fig.colorbar(inun_im, ax=ax[1], shrink=0.4)\n",
        "ax[1].set_title(\"Inundation map from NWM forecast\")\n",
        "ax[1].set_xlabel('x (m)')\n",
        "ax[1].set_ylabel('y (m)')\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}